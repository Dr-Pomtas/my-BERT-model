import os
import pandas as pd
import numpy as np
from flask import Flask, render_template, request, jsonify, send_file
import plotly
import plotly.graph_objs as go
import plotly.express as px
from sklearn.metrics import mean_absolute_error
from scipy.stats import pearsonr
from scipy import stats
import json
import re
import io
import base64
from werkzeug.utils import secure_filename
import random
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

def convert_numpy_types(obj):
    """numpyå‹ã‚’Pythonãƒã‚¤ãƒ†ã‚£ãƒ–å‹ã«å¤‰æ›"""
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.bool_, bool)):
        return bool(obj)
    else:
        return obj
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
uploaded_data = None
analysis_results = None

# ä½¿ç”¨ã™ã‚‹3ã¤ã®ãƒ¢ãƒ‡ãƒ«
MODELS = {
    'koheiduck/bert-japanese-finetuned-sentiment': 'Model A (Koheiduck)',
    'llm-book/bert-base-japanese-v2-finetuned-sentiment': 'Model B (LLM-book)', 
    'Mizuiro-inc/bert-base-japanese-finetuned-sentiment-analysis': 'Model C (Mizuiro)'
}

class SentimentAnalyzer:
    def __init__(self):
        self.models = {}
        # ãƒ¢ãƒƒã‚¯ç”¨ã®ã‚·ãƒ¼ãƒ‰è¨­å®šï¼ˆå†ç¾å¯èƒ½ãªçµæœã®ãŸã‚ï¼‰
        np.random.seed(42)
        random.seed(42)
    
    def preprocess_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
        if pd.isna(text):
            return ""
        
        # æ–‡å­—åˆ—ã«å¤‰æ›
        text = str(text)
        
        # URLé™¤å»
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # çµµæ–‡å­—ã¨ç‰¹æ®Šè¨˜å·ã®é™¤å»ï¼ˆåŸºæœ¬çš„ãªæ—¥æœ¬èªæ–‡å­—ã€è‹±æ•°å­—ã€åŸºæœ¬çš„ãªè¨˜å·ã®ã¿æ®‹ã™ï¼‰
        text = re.sub(r'[^\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u3400-\u4DBFa-zA-Z0-9\sã€‚ã€ï¼ï¼Ÿ\.,!?()ï¼ˆï¼‰\-]', '', text)
        
        # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã®ç©ºç™½ã«
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def analyze_sentiment(self, text, model_name):
        """æ„Ÿæƒ…åˆ†æã‚’ãƒ¢ãƒƒã‚¯å®Ÿè¡Œï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰"""
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        processed_text = self.preprocess_text(text)
        if not processed_text:
            return {'positive': 0.5, 'negative': 0.5}
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ãŸãƒ¢ãƒƒã‚¯æ„Ÿæƒ…åˆ†æ
            text_len = len(processed_text)
            
            # ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
            positive_words = ['è‰¯ã„', 'ã‚ˆã„', 'è¦ªåˆ‡', 'ä¸å¯§', 'å®‰å¿ƒ', 'ç´ æ™´ã‚‰ã—ã„', 'å„ªã—ã„', 'æ¸…æ½”', 'çš„ç¢º', 'é ¼ã‚Š']
            negative_words = ['æ‚ªã„', 'ã‚ã‚‹ã„', 'é«˜ã„', 'é•·ã„', 'ç‹­ã„', 'ä¸ä¾¿', 'ä¸ååˆ†', 'å¤ã„', 'ä¸å®‰']
            
            positive_count = sum(1 for word in positive_words if word in processed_text)
            negative_count = sum(1 for word in negative_words if word in processed_text)
            
            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹ç‰¹æ€§ã‚’æŒãŸã›ã‚‹
            model_variants = {
                'koheiduck/bert-japanese-finetuned-sentiment': 0.0,
                'llm-book/bert-base-japanese-v2-finetuned-sentiment': 0.1, 
                'Mizuiro-inc/bert-base-japanese-finetuned-sentiment-analysis': -0.05
            }
            
            variant = model_variants.get(model_name, 0.0)
            
            # åŸºæœ¬ã‚¹ã‚³ã‚¢è¨ˆç®—
            base_positive = 0.4 + (positive_count * 0.15) - (negative_count * 0.1) + variant
            base_negative = 0.4 + (negative_count * 0.15) - (positive_count * 0.1) - variant
            
            # æ­£è¦åŒ–ï¼ˆ0-1ã®ç¯„å›²ã«åã‚ã‚‹ï¼‰
            total = base_positive + base_negative
            if total > 0:
                positive_prob = base_positive / total
                negative_prob = base_negative / total
            else:
                positive_prob = 0.5
                negative_prob = 0.5
            
            # ã‚ãšã‹ãªãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’è¿½åŠ ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰
            text_hash = hash(processed_text + model_name) % 1000
            noise = (text_hash / 1000 - 0.5) * 0.1
            
            positive_prob = max(0.0, min(1.0, positive_prob + noise))
            negative_prob = 1.0 - positive_prob
            
            return {
                'positive': positive_prob,
                'negative': negative_prob
            }
            
        except Exception as e:
            print(f"æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼ ({model_name}): {str(e)}")
            return {'positive': 0.5, 'negative': 0.5}

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
analyzer = SentimentAnalyzer()

def calculate_scores(data):
    """å…¨ãƒ¢ãƒ‡ãƒ«ã§ã®æ„Ÿæƒ…åˆ†æã¨ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    results = {}
    
    for model_name, display_name in MODELS.items():
        print(f"ãƒ¢ãƒ‡ãƒ« {display_name} ã§ã®åˆ†æé–‹å§‹...")
        
        model_scores = []
        for idx, row in data.iterrows():
            sentiment = analyzer.analyze_sentiment(row['review_text'], model_name)
            if sentiment:
                # å£ã‚³ãƒŸã‚¹ã‚³ã‚¢è¨ˆç®—: (P(pos) * 2) - (P(neg) * 2)
                review_score = (sentiment['positive'] * 2) - (sentiment['negative'] * 2)
                model_scores.append(review_score)
                # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®3ä»¶ã®åˆ†æçµæœã‚’å‡ºåŠ›
                if idx < 3:
                    print(f"  ã‚µãƒ³ãƒ—ãƒ« {idx}: text='{row['review_text'][:30]}...', pos={sentiment['positive']:.3f}, neg={sentiment['negative']:.3f}, score={review_score:.3f}")
            else:
                model_scores.append(0.0)
                if idx < 3:
                    print(f"  ã‚µãƒ³ãƒ—ãƒ« {idx}: æ„Ÿæƒ…åˆ†æå¤±æ•—")
        
        print(f"  {display_name} ã‚¹ã‚³ã‚¢ç¯„å›²: min={min(model_scores):.3f}, max={max(model_scores):.3f}, avg={sum(model_scores)/len(model_scores):.3f}")
        
        data[f'{display_name}_score'] = model_scores
        print(f"ãƒ¢ãƒ‡ãƒ« {display_name} ã®åˆ†æå®Œäº†")
    
    # æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢æ­£è¦åŒ–: (1-5) â†’ (-2 to +2)
    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ•°å€¤ã«å¤‰æ›ã—ã¦ã‹ã‚‰è¨ˆç®—
    try:
        data['star_rating'] = pd.to_numeric(data['star_rating'], errors='coerce')
        data['star_score'] = data['star_rating'] - 3
        print(f"æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢æ­£è¦åŒ–å®Œäº†: {data['star_score'].dtype}")
        print(f"DEBUG: star_rating range: {data['star_rating'].min()} to {data['star_rating'].max()}")
        print(f"DEBUG: star_score range: {data['star_score'].min()} to {data['star_score'].max()}")
        print(f"DEBUG: First 5 star_score values: {data['star_score'].head().tolist()}")
    except Exception as e:
        print(f"æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢æ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—åˆ—ã‹ã‚‰æ•°å€¤ã¸ã®å¤‰æ›ã‚’è©¦è¡Œ
        data['star_rating'] = data['star_rating'].astype(str).str.extract(r'(\d+)').astype(float)
        data['star_score'] = data['star_rating'] - 3
        print(f"DEBUG FALLBACK: star_score range: {data['star_score'].min()} to {data['star_score'].max()}")
    
    return data

def aggregate_by_hospital(data):
    """ç—…é™¢å˜ä½ã§ã®é›†è¨ˆ"""
    # ç—…é™¢IDã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦å¹³å‡ã‚’è¨ˆç®—
    hospital_stats = data.groupby('hospital_id').agg({
        'star_score': 'mean',
        **{f'{display_name}_score': 'mean' for display_name in MODELS.values()}
    }).reset_index()
    
    # å„ç—…é™¢ã®å£ã‚³ãƒŸæ•°ã‚‚è¿½åŠ 
    review_counts = data.groupby('hospital_id').size().reset_index(name='review_count')
    hospital_stats = hospital_stats.merge(review_counts, on='hospital_id')
    
    return hospital_stats

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/debug')
def debug():
    return render_template('debug.html')

@app.route('/full')
def full_featured():
    return render_template('full_featured.html')

@app.route('/test')
def test_js():
    return render_template('test_js.html')

@app.route('/export_results', methods=['POST'])
def export_results():
    global analysis_results, uploaded_data
    
    print("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹...")
    
    print(f"ãƒ‡ãƒãƒƒã‚°: analysis_results = {analysis_results is not None}")
    print(f"ãƒ‡ãƒãƒƒã‚°: uploaded_data = {uploaded_data is not None}")
    if analysis_results is not None:
        print(f"ãƒ‡ãƒãƒƒã‚°: analysis_results keys = {analysis_results.keys()}")
    
    if analysis_results is None or uploaded_data is None:
        print("ã‚¨ãƒ©ãƒ¼: åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“")
        print(f"  analysis_results: {analysis_results}")
        print(f"  uploaded_data: {uploaded_data}")
        return jsonify({'error': 'åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“'}), 400
    
    try:
        # åˆ†ææ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        scored_data = analysis_results['scored_data']
        if not isinstance(scored_data, pd.DataFrame):
            scored_data = pd.DataFrame(scored_data)
        print(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡ãƒ‡ãƒ¼ã‚¿æ•°: {len(scored_data)}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆ10ä¸‡ä»¶ã¾ã§ï¼‰
        if len(scored_data) > 100000:
            print(f"è­¦å‘Š: ãƒ‡ãƒ¼ã‚¿ãŒå¤§ãã™ãã¾ã™ ({len(scored_data)}ä»¶)")
            return jsonify({'error': 'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ã€‚10ä¸‡ä»¶ä»¥ä¸‹ã«åˆ¶é™ã—ã¦ãã ã•ã„ã€‚'}), 400
        
        # CSVç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
        print("CSVç”¨ãƒ‡ãƒ¼ã‚¿æ•´ç†é–‹å§‹...")
        
        # åˆ—åã®ãƒãƒƒãƒ”ãƒ³ã‚°
        export_columns = {
            'hospital_id': 'ç—…é™¢ID',
            'review_text': 'ãƒ¬ãƒ“ãƒ¥ãƒ¼æ–‡', 
            'star_rating': 'æ˜Ÿè©•ä¾¡',
            'star_score': 'æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢',
            'Model A (Koheiduck)_score': 'Koheiduckæ„Ÿæƒ…ã‚¹ã‚³ã‚¢',
            'Model B (LLM-book)_score': 'LLM-bookæ„Ÿæƒ…ã‚¹ã‚³ã‚¢', 
            'Model C (Mizuiro)_score': 'Mizuiroæ„Ÿæƒ…ã‚¹ã‚³ã‚¢'
        }
        
        # å¿…è¦ãªåˆ—ã®ã¿ã‚’é¸æŠã—ã¦ãƒªãƒãƒ¼ãƒ 
        df_export = scored_data[list(export_columns.keys())].rename(columns=export_columns)
        print(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(df_export)}è¡Œ, {len(df_export.columns)}åˆ—")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ï¼ˆBOMä»˜ãUTF-8ã§ç¢ºå®Ÿã«æ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
        output = io.StringIO()
        df_export.to_csv(output, index=False)  # pandasã®to_csvã§encodingãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä½¿ã‚ãªã„
        csv_string = output.getvalue()
        output.close()
        
        # BOMã‚’æ‰‹å‹•ã§è¿½åŠ ï¼ˆExcelç­‰ã§ã®æ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
        csv_bytes = '\ufeff' + csv_string
        csv_encoded = csv_bytes.encode('utf-8')
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¨ã—ã¦è¿”ã™
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"animal_hospital_review_analysis_{timestamp}.csv"
        
        response = app.response_class(
            csv_encoded,
            mimetype='application/octet-stream',
            headers={
                'Content-Disposition': f'attachment; filename*=UTF-8\'\'{filename}',
                'Content-Type': 'application/octet-stream',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
        )
        
        return response
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼è©³ç´°: {error_details}")
        return jsonify({'error': f'ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/upload', methods=['POST'])
def upload_file():
    global uploaded_data
    
    if 'file' not in request.files:
        return jsonify({'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
    
    if file and file.filename.endswith('.csv'):
        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv(file)
            
            # å¿…è¦ãªåˆ—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            required_columns = ['hospital_id', 'review_text', 'star_rating']
            if not all(col in df.columns for col in required_columns):
                return jsonify({'error': f'å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {required_columns}'}), 400
            
            # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
            df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')
            df = df.dropna(subset=['star_rating'])
            df['star_rating'] = df['star_rating'].astype(int)
            
            # 1-5ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
            df = df[(df['star_rating'] >= 1) & (df['star_rating'] <= 5)]
            
            uploaded_data = df
            
            # åŸºæœ¬çµ±è¨ˆé‡è¨ˆç®—
            total_reviews = len(df)
            unique_hospitals = df['hospital_id'].nunique()
            avg_star_rating = float(df['star_rating'].mean())
            star_distribution = df['star_rating'].value_counts().sort_index().to_dict()
            
            print(f"ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ: ç·å£ã‚³ãƒŸæ•°={total_reviews}, ç—…é™¢æ•°={unique_hospitals}, å¹³å‡æ˜Ÿè©•ä¾¡={avg_star_rating:.2f}")
            print(f"ç—…é™¢IDä¸€è¦§: {df['hospital_id'].unique().tolist()}")
            
            stats = {
                'total_reviews': total_reviews,
                'unique_hospitals': unique_hospitals,
                'avg_star_rating': avg_star_rating,
                'star_distribution': star_distribution
            }
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
            data_for_js = df.to_dict('records')
            
            return jsonify({
                'success': True, 
                'stats': stats,
                'data': data_for_js
            })
            
        except Exception as e:
            return jsonify({'error': f'ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500
    
    return jsonify({'error': 'ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„'}), 400

@app.route('/analyze', methods=['POST'])
def analyze():
    global analysis_results, uploaded_data
    
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    request_data = request.get_json()
    if not request_data or 'data' not in request_data:
        return jsonify({'error': 'ãƒ‡ãƒ¼ã‚¿ãŒé€ä¿¡ã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
    uploaded_data = pd.DataFrame(request_data['data'])
    
    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’é©åˆ‡ã«å¤‰æ›
    print(f"å—ä¿¡ãƒ‡ãƒ¼ã‚¿æ•°: {len(uploaded_data)}")
    print(f"ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª - star_rating: {uploaded_data['star_rating'].dtype}")
    
    # star_ratingã‚’ç¢ºå®Ÿã«æ•°å€¤å‹ã«å¤‰æ›
    try:
        uploaded_data['star_rating'] = pd.to_numeric(uploaded_data['star_rating'], errors='coerce')
        print(f"å¤‰æ›å¾Œ - star_rating: {uploaded_data['star_rating'].dtype}")
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': f'ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 400
    
    try:
        # æ„Ÿæƒ…åˆ†æã¨ã‚¹ã‚³ã‚¢è¨ˆç®—
        scored_data = calculate_scores(uploaded_data.copy())
        
        # ç—…é™¢å˜ä½ã§é›†è¨ˆ
        hospital_stats = aggregate_by_hospital(scored_data)
        
        print(f"é›†è¨ˆå¾Œã®ç—…é™¢æ•°: {len(hospital_stats)}")
        print(f"é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«:")
        print(hospital_stats.head())
        
        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡
        performance_metrics = {}
        
        for model_name, display_name in MODELS.items():
            model_col = f'{display_name}_score'
            
            # MAEè¨ˆç®—ï¼ˆç—…é™¢å˜ä½ã§ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
            mae = mean_absolute_error(hospital_stats['star_score'], hospital_stats[model_col])
            
            # ä¸€æ™‚çš„ãªperformance_metricsï¼ˆç›¸é–¢ä¿‚æ•°ã¯å¾Œã§å…¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ãã™ã‚‹ï¼‰
            performance_metrics[display_name] = {
                'correlation': 0.0,  # å¾Œã§ä¸Šæ›¸ã
                'p_value': 0.0,      # å¾Œã§ä¸Šæ›¸ã
                'mae': float(mae)
            }
        
        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒã®ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ¤œå®š (10000å›)
        model_performance_tests = {}
        model_names_list = list(MODELS.values())
        
        for i, model1 in enumerate(model_names_list):
            for j, model2 in enumerate(model_names_list):
                if i < j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚
                    col1 = f'{model1}_score'
                    col2 = f'{model2}_score'
                    
                    if col1 in hospital_stats.columns and col2 in hospital_stats.columns:
                        # MAEå·®ã®ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ¤œå®š
                        test_results = bootstrap_mae_difference_test(
                            hospital_stats['star_score'].tolist(),
                            hospital_stats[col1].tolist(),
                            hospital_stats[col2].tolist(),
                            n_bootstrap=10000
                        )
                        
                        model_performance_tests[f'{model1}_vs_{model2}'] = {
                            'mae_difference': test_results['mean_difference'],
                            'p_value': test_results['p_value'],
                            'ci_lower': test_results['confidence_interval'][0],
                            'ci_upper': test_results['confidence_interval'][1],
                            'significant': bool(test_results['p_value'] < 0.05)
                        }
        
        # æ˜Ÿè©•ä¾¡åˆ†å¸ƒã®è¨ˆç®—
        star_distribution = uploaded_data['star_rating'].value_counts().sort_index().to_dict()
        
        # æ˜Ÿè©•ä¾¡ã¨æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®ç›¸é–¢åˆ†æ
        sentiment_correlation_data = {}
        scatter_data = {}
        correlation_results = {}
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®æ˜Ÿè©•ä¾¡ã¨ã®ç›¸é–¢ã‚’è¨ˆç®—
        for model_name, display_name in MODELS.items():
            model_col = f'{display_name}_score'
            
            # æ•£å¸ƒå›³ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆå¼·åˆ¶çš„ã«æ­£è¦åŒ–: 1-5 â†’ -2~+2ï¼‰
            original_ratings = scored_data['star_rating'].tolist()
            normalized_ratings = [(rating - 3) for rating in original_ratings]  # ç¢ºå®Ÿã«æ­£è¦åŒ–
            
            print(f"Debug: {display_name} original star_rating range: {min(original_ratings)} to {max(original_ratings)}")
            print(f"Debug: {display_name} normalized star_score range: {min(normalized_ratings)} to {max(normalized_ratings)}")
            print(f"Debug: First 5 normalized star_scores: {normalized_ratings[:5]}")
            
            scatter_data[display_name] = {
                'star_ratings': normalized_ratings,  # ç¢ºå®Ÿã«æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
                'sentiment_scores': scored_data[model_col].tolist()
            }
            
            # ç›¸é–¢ä¿‚æ•°ã¨æ¤œå®šï¼ˆæ­£è¦åŒ–å¾Œã®æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢ã§è¨ˆç®—ï¼‰
            correlation, p_value = pearsonr(scored_data['star_score'], scored_data[model_col])
            
            # performance_metricsã‚’æ­£ã—ã„ç›¸é–¢ä¿‚æ•°ã§æ›´æ–°
            performance_metrics[display_name].update({
                'correlation': float(correlation),
                'p_value': float(p_value)
            })
            
            # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ä¿¡é ¼åŒºé–“ (10000å›) - æ­£è¦åŒ–å¾Œã®æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
            bootstrap_result = bootstrap_correlation_ci(
                scored_data['star_score'].tolist(), 
                scored_data[model_col].tolist(), 
                n_bootstrap=10000
            )
            
            correlation_results[display_name] = {
                'correlation': float(correlation),
                'p_value': float(p_value),
                'ci_lower': float(bootstrap_result['ci_lower']),
                'ci_upper': float(bootstrap_result['ci_upper']),
                'significant': bool(p_value < 0.05),
                'sample_size': len(uploaded_data)
            }
        
        sentiment_correlation_data = {
            'scatter_data': scatter_data,
            'correlations': correlation_results
        }
        
        # åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—
        review_lengths = uploaded_data['review_text'].str.len()
        star_ratings = uploaded_data['star_rating']
        
        basic_stats = {
            'total_reviews': len(uploaded_data),
            'unique_hospitals': len(hospital_stats),
            'avg_rating': float(star_ratings.mean()),
            'avg_review_length': float(review_lengths.mean()),
            'rating_std': float(star_ratings.std()),
            'length_std': float(review_lengths.std()),
            'min_rating': int(star_ratings.min()),
            'max_rating': int(star_ratings.max()),
            'median_rating': float(star_ratings.median()),
            'min_length': int(review_lengths.min()),
            'max_length': int(review_lengths.max())
        }
        
        # ç—…é™¢åˆ¥åˆ†æãƒ‡ãƒ¼ã‚¿
        hospital_analysis = {}
        for _, row in hospital_stats.iterrows():
            hospital_id = row['hospital_id']
            
            # å„ãƒ¢ãƒ‡ãƒ«ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’å€‹åˆ¥ã«å–å¾—
            sentiment_scores = {}
            sentiment_values = []
            for model_name, display_name in MODELS.items():
                score_col = f'{display_name}_score'
                if score_col in row.index:
                    score_value = float(row[score_col])
                    sentiment_scores[display_name] = score_value
                    sentiment_values.append(score_value)
            
            # å¹³å‡æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
            avg_sentiment = sum(sentiment_values) / len(sentiment_values) if sentiment_values else 0.0
            
            hospital_analysis[hospital_id] = {
                'review_count': int(row['review_count']),
                'avg_rating': float(scored_data[scored_data['hospital_id'] == hospital_id]['star_score'].mean()),
                'avg_sentiment': avg_sentiment,
                'model_sentiments': sentiment_scores,
                # JavaScriptç”¨ã«ç›´æ¥çš„ãªã‚­ãƒ¼ã‚‚è¿½åŠ 
                **sentiment_scores  # è¾æ›¸ã‚’å±•é–‹ã—ã¦ã‚­ãƒ¼ã‚’ç›´æ¥è¿½åŠ 
            }
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«åˆ†æçµæœã‚’ä¿å­˜ï¼ˆCSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ï¼‰
        analysis_results = {
            'hospital_stats': hospital_stats,
            'performance_metrics': performance_metrics,
            'scored_data': scored_data
        }
        # å…ƒã®uploaded_dataã‚‚ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜
        uploaded_data = pd.DataFrame(request_data['data'])
        
        # JavaScriptãŒæœŸå¾…ã™ã‚‹å½¢å¼ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        response_data = {
            'success': True,
            'results': {
                'basic_stats': basic_stats,
                'model_comparison': performance_metrics,
                'star_rating_distribution': star_distribution,
                'sentiment_correlation': sentiment_correlation_data,
                'hospital_analysis': hospital_analysis,
                'model_performance_tests': model_performance_tests
            }
        }
        
        # numpyå‹ã‚’Pythonãƒã‚¤ãƒ†ã‚£ãƒ–å‹ã«å¤‰æ›
        response_data = convert_numpy_types(response_data)
        
        return jsonify(response_data)
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"åˆ†æã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {error_details}")
        return jsonify({'error': f'åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

def bootstrap_correlation_ci(x_data, y_data, n_bootstrap=10000, confidence_level=0.95):
    """ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³•ã§ç›¸é–¢ä¿‚æ•°ã®ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—"""
    n = len(x_data)
    bootstrap_correlations = []
    
    for _ in range(n_bootstrap):
        # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        indices = np.random.choice(n, n, replace=True)
        x_boot = [x_data[i] for i in indices]
        y_boot = [y_data[i] for i in indices]
        
        # ç›¸é–¢ä¿‚æ•°è¨ˆç®—
        if len(set(x_boot)) > 1 and len(set(y_boot)) > 1:  # åˆ†æ•£ãŒ0ã§ãªã„å ´åˆã®ã¿
            corr, _ = pearsonr(x_boot, y_boot)
            if not np.isnan(corr):
                bootstrap_correlations.append(corr)
    
    # ä¿¡é ¼åŒºé–“è¨ˆç®—
    alpha = 1 - confidence_level
    lower_percentile = (alpha / 2) * 100
    upper_percentile = (1 - alpha / 2) * 100
    
    ci_lower = np.percentile(bootstrap_correlations, lower_percentile)
    ci_upper = np.percentile(bootstrap_correlations, upper_percentile)
    
    return {
        'ci_lower': ci_lower,
        'ci_upper': ci_upper,
        'correlations': bootstrap_correlations
    }

def bootstrap_mae_difference_test(y_true, y_pred1, y_pred2, n_bootstrap=10000, confidence_level=0.95):
    """ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³•ã§MAEå·®ã®ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—"""
    n = len(y_true)
    mae_differences = []
    
    print(f"çµ±è¨ˆæ¤œå®šé–‹å§‹: ãƒ¢ãƒ‡ãƒ«1 vs ãƒ¢ãƒ‡ãƒ«2")
    print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {n}")
    
    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®MAEè¨ˆç®—
    original_mae1 = mean_absolute_error(y_true, y_pred1)
    original_mae2 = mean_absolute_error(y_true, y_pred2)
    print(f"ã‚ªãƒªã‚¸ãƒŠãƒ«MAE: ãƒ¢ãƒ‡ãƒ«1={original_mae1:.4f}, ãƒ¢ãƒ‡ãƒ«2={original_mae2:.4f}")
    
    for i in range(n_bootstrap):
        # é€²è¡ŒçŠ¶æ³è¡¨ç¤ºï¼ˆ1000å›ã”ã¨ï¼‰
        if (i + 1) % 1000 == 0:
            print(f"ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—é€²æ—: {i+1}/{n_bootstrap}")
            
        # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        indices = np.random.choice(n, n, replace=True)
        y_true_boot = [y_true[i] for i in indices]
        y_pred1_boot = [y_pred1[i] for i in indices]
        y_pred2_boot = [y_pred2[i] for i in indices]
        
        # MAEè¨ˆç®—
        mae1 = mean_absolute_error(y_true_boot, y_pred1_boot)
        mae2 = mean_absolute_error(y_true_boot, y_pred2_boot)
        
        # MAEå·®ã‚’è¨˜éŒ² (ãƒ¢ãƒ‡ãƒ«2ã®MAE - ãƒ¢ãƒ‡ãƒ«1ã®MAE)
        mae_differences.append(mae2 - mae1)
    
    # ä¿¡é ¼åŒºé–“è¨ˆç®—
    alpha = 1 - confidence_level
    lower_percentile = (alpha / 2) * 100
    upper_percentile = (1 - alpha / 2) * 100
    
    ci_lower = np.percentile(mae_differences, lower_percentile)
    ci_upper = np.percentile(mae_differences, upper_percentile)
    
    # på€¤è¨ˆç®—ï¼ˆä¸¡å´æ¤œå®šï¼‰
    mean_difference = original_mae1 - original_mae2
    p_value = np.sum(np.abs(mae_differences) >= np.abs(mean_difference)) / len(mae_differences)
    
    print(f"95%ä¿¡é ¼åŒºé–“: [{ci_lower:.4f}, {ci_upper:.4f}]")
    
    return {
        'mean_difference': mean_difference,
        'p_value': p_value,
        'confidence_interval': [ci_lower, ci_upper],
        'mae_differences': mae_differences
    }

@app.route('/get_charts')
def get_charts():
    global analysis_results
    
    if analysis_results is None:
        return jsonify({'error': 'åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“'}), 400
    
    try:
        hospital_stats = analysis_results['hospital_stats']
        performance_metrics = analysis_results['performance_metrics']
        
        # 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•ï¼ˆç›¸é–¢ä¿‚æ•°ï¼‰- ä¿¡é ¼åŒºé–“ä»˜ã
        models = list(performance_metrics.keys())
        correlations = []
        correlation_cis = []
        
        for model in models:
            model_col = f'{model}_score'
            x_data = hospital_stats[model_col].tolist()
            y_data = hospital_stats['star_score'].tolist()
            
            # ç›¸é–¢ä¿‚æ•°ã®ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—
            bootstrap_result = bootstrap_correlation_ci(x_data, y_data)
            ci_lower = bootstrap_result['ci_lower']
            ci_upper = bootstrap_result['ci_upper']
            correlation_cis.append((ci_lower, ci_upper))
            correlations.append(performance_metrics[model]['correlation'])
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãã®ç›¸é–¢ä¿‚æ•°ã‚°ãƒ©ãƒ•
        correlation_chart = go.Figure(data=[
            go.Bar(
                x=models, 
                y=correlations, 
                name='ç›¸é–¢ä¿‚æ•°',
                error_y=dict(
                    type='data',
                    symmetric=False,
                    array=[ci[1] - corr for ci, corr in zip(correlation_cis, correlations)],
                    arrayminus=[corr - ci[0] for ci, corr in zip(correlation_cis, correlations)]
                )
            )
        ])
        correlation_chart.update_layout(
            title='ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ: ç›¸é–¢ä¿‚æ•° (95%ä¿¡é ¼åŒºé–“ä»˜ã)',
            xaxis_title='ãƒ¢ãƒ‡ãƒ«',
            yaxis_title='ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°',
            showlegend=False
        )
        
        # 2. MAEæ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•
        mae_values = [performance_metrics[model]['mae'] for model in models]
        
        mae_chart = go.Figure(data=[
            go.Bar(x=models, y=mae_values, name='MAE', marker_color='orange')
        ])
        mae_chart.update_layout(
            title='ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ: å¹³å‡çµ¶å¯¾èª¤å·® (MAE)',
            xaxis_title='ãƒ¢ãƒ‡ãƒ«',
            yaxis_title='å¹³å‡çµ¶å¯¾èª¤å·®',
            showlegend=False
        )
        
        # 3. æ•£å¸ƒå›³ï¼ˆå„ãƒ¢ãƒ‡ãƒ«ï¼‰- ä¿¡é ¼åŒºé–“æƒ…å ±ä»˜ã
        scatter_charts = []
        
        for i, (model_name, display_name) in enumerate(MODELS.items()):
            model_col = f'{display_name}_score'
            correlation = performance_metrics[display_name]['correlation']
            ci_lower, ci_upper = correlation_cis[i]
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ç¢ºå®Ÿã«ãƒ—ãƒ­ãƒƒãƒˆ
            x_data = hospital_stats[model_col].tolist()
            y_data = hospital_stats['star_score'].tolist()
            hospital_ids = hospital_stats['hospital_id'].tolist()
            
            scatter_chart = go.Figure()
            
            scatter_chart.add_trace(go.Scatter(
                x=x_data,
                y=y_data,
                mode='markers',
                marker=dict(
                    size=10, 
                    opacity=0.7,
                    color='blue',
                    line=dict(width=1, color='darkblue')
                ),
                text=[f'ç—…é™¢ID: {hid}' for hid in hospital_ids],
                hovertemplate='<b>%{text}</b><br>å£ã‚³ãƒŸã‚¹ã‚³ã‚¢: %{x:.3f}<br>æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢: %{y:.3f}<extra></extra>',
                name='ç—…é™¢ãƒ‡ãƒ¼ã‚¿'
            ))
            
            # å›å¸°ç·šã‚’è¿½åŠ 
            if len(x_data) > 1:
                slope, intercept, r_value, p_value, std_err = stats.linregress(x_data, y_data)
                x_line = [min(x_data), max(x_data)]
                y_line = [slope * x + intercept for x in x_line]
                
                scatter_chart.add_trace(go.Scatter(
                    x=x_line,
                    y=y_line,
                    mode='lines',
                    line=dict(color='red', width=2),
                    name=f'å›å¸°ç·š (r={correlation:.3f})',
                    showlegend=True
                ))
            
            scatter_chart.update_layout(
                title=f'{display_name}<br>ç›¸é–¢ä¿‚æ•° r = {correlation:.3f} (95%CI: [{ci_lower:.3f}, {ci_upper:.3f}])<br>ç—…é™¢æ•°: {len(hospital_stats)}',
                xaxis_title='å¹³å‡å£ã‚³ãƒŸã‚¹ã‚³ã‚¢',
                yaxis_title='å¹³å‡æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢',
                width=400,
                height=400,
                showlegend=True
            )
            
            scatter_charts.append(scatter_chart)
        
        # MAEã§ãƒ¢ãƒ‡ãƒ«ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠç”¨ã®æƒ…å ±ã‚’è¿½åŠ 
        mae_sorted_models = sorted(models, key=lambda m: performance_metrics[m]['mae'])
        
        # ãƒãƒ£ãƒ¼ãƒˆã‚’JSONã«å¤‰æ›
        charts_json = {
            'correlation_chart': json.dumps(correlation_chart, cls=plotly.utils.PlotlyJSONEncoder),
            'mae_chart': json.dumps(mae_chart, cls=plotly.utils.PlotlyJSONEncoder),
            'scatter_charts': [json.dumps(chart, cls=plotly.utils.PlotlyJSONEncoder) for chart in scatter_charts],
            'model_list': models,
            'best_model': mae_sorted_models[0] if mae_sorted_models else models[0],
            'second_best_model': mae_sorted_models[1] if len(mae_sorted_models) > 1 else models[1] if len(models) > 1 else models[0],
            'performance_metrics': performance_metrics,
            'correlation_cis': {model: {'lower': ci[0], 'upper': ci[1]} for model, ci in zip(models, correlation_cis)}
        }
        
        return jsonify(charts_json)
        
    except Exception as e:
        return jsonify({'error': f'ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/get_performance_metrics')
def get_performance_metrics():
    global analysis_results
    
    if analysis_results is None:
        return jsonify({'error': 'åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“'}), 400
    
    return jsonify({
        'success': True,
        'performance_metrics': analysis_results['performance_metrics']
    })

@app.route('/statistical_test', methods=['POST'])
def statistical_test():
    global analysis_results
    
    if analysis_results is None:
        return jsonify({'error': 'åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“'}), 400
    
    try:
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰æ¯”è¼ƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        data = request.get_json()
        model1 = data.get('model1')
        model2 = data.get('model2')
        
        if not model1 or not model2:
            return jsonify({'error': 'ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
        
        if model1 == model2:
            return jsonify({'error': 'ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„'}), 400
        
        hospital_stats = analysis_results['hospital_stats']
        
        # ãƒ¢ãƒ‡ãƒ«ã®ã‚«ãƒ©ãƒ åã‚’ç”Ÿæˆ
        model1_col = f'{model1}_score'
        model2_col = f'{model2}_score'
        
        # ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if model1_col not in hospital_stats.columns or model2_col not in hospital_stats.columns:
            return jsonify({'error': 'ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}), 400
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        star_scores = hospital_stats['star_score'].values
        model1_scores = hospital_stats[model1_col].values
        model2_scores = hospital_stats[model2_col].values
        
        print(f"çµ±è¨ˆæ¤œå®šé–‹å§‹: {model1} vs {model2}")
        print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(star_scores)}")
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®MAEã‚’è¨ˆç®—
        mae1 = mean_absolute_error(star_scores, model1_scores)
        mae2 = mean_absolute_error(star_scores, model2_scores)
        
        print(f"ã‚ªãƒªã‚¸ãƒŠãƒ«MAE: {model1}={mae1:.4f}, {model2}={mae2:.4f}")
        
        # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³•ã«ã‚ˆã‚‹æ¤œå®š
        bootstrap_iterations = 10000
        mae_differences = []
        
        np.random.seed(42)  # å†ç¾å¯èƒ½ãªçµæœã®ãŸã‚
        
        for i in range(bootstrap_iterations):
            # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            n = len(star_scores)
            indices = np.random.choice(n, size=n, replace=True)
            
            # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§MAEè¨ˆç®—
            boot_star = star_scores[indices]
            boot_model1 = model1_scores[indices]
            boot_model2 = model2_scores[indices]
            
            boot_mae1 = mean_absolute_error(boot_star, boot_model1)
            boot_mae2 = mean_absolute_error(boot_star, boot_model2)
            
            # MAEã®å·®ã‚’è¨˜éŒ²
            mae_diff = boot_mae2 - boot_mae1
            mae_differences.append(mae_diff)
            
            # é€²æ—è¡¨ç¤ºï¼ˆ1000å›ã”ã¨ï¼‰
            if (i + 1) % 1000 == 0:
                print(f"ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—é€²æ—: {i + 1}/{bootstrap_iterations}")
        
        # 95%ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—
        mae_differences = np.array(mae_differences)
        confidence_interval = [
            float(np.percentile(mae_differences, 2.5)),
            float(np.percentile(mae_differences, 97.5))
        ]
        
        print(f"95%ä¿¡é ¼åŒºé–“: [{confidence_interval[0]:.4f}, {confidence_interval[1]:.4f}]")
        
        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        result = {
            'model1': model1,
            'model2': model2,
            'mae1': float(mae1),
            'mae2': float(mae2),
            'mae_difference': float(mae2 - mae1),
            'confidence_interval': confidence_interval,
            'bootstrap_iterations': bootstrap_iterations,
            'is_significant': confidence_interval[0] > 0 or confidence_interval[1] < 0
        }
        
        return jsonify({'success': True, 'result': result})
        
    except Exception as e:
        print(f"çµ±è¨ˆæ¤œå®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({'error': f'çµ±è¨ˆæ¤œå®šã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

# Removed duplicate export_results endpoint - clean section
        new_columns = ['Hospital_ID', 'Review_Text', 'Star_Rating']
        for i, (model_name, display_name) in enumerate(MODELS.items()):
            if f'{display_name}_score' in model_columns:
                new_columns.append(f'{display_name.replace(" ", "_").replace("(", "").replace(")", "")}_Sentiment_Score')
        
        export_data.columns = new_columns
        
# Clean section - old code completely removed

@app.route('/download_sample')
def download_sample():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        sample_file_path = os.path.join(os.path.dirname(__file__), 'sample_data.csv')
        
        if not os.path.exists(sample_file_path):
            return jsonify({'error': 'ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}), 404
        
        return send_file(
            sample_file_path,
            mimetype='text/csv',
            as_attachment=True,
            download_name='å‹•ç‰©ç—…é™¢å£ã‚³ãƒŸã‚µãƒ³ãƒ—ãƒ«.csv'
        )
        
    except Exception as e:
        return jsonify({'error': f'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/load_sample_data', methods=['GET'])
def load_sample_data():
    """JavaScriptç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ï¼ˆGETãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰"""
    global uploaded_data
    
    try:
        sample_file_path = os.path.join(os.path.dirname(__file__), 'sample_data.csv')
        
        if not os.path.exists(sample_file_path):
            return jsonify({'success': False, 'error': 'ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}), 404
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        df = pd.read_csv(sample_file_path, encoding='utf-8')
        
        # å¿…è¦ãªåˆ—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        required_columns = ['hospital_id', 'review_text', 'star_rating']
        if not all(col in df.columns for col in required_columns):
            return jsonify({'success': False, 'error': f'å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {required_columns}'}), 400
        
        # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')
        df = df.dropna(subset=['star_rating'])
        df['star_rating'] = df['star_rating'].astype(int)
        
        # uploaded_data ã«ã‚»ãƒƒãƒˆ
        uploaded_data = df
        
        # åŸºæœ¬çµ±è¨ˆé‡è¨ˆç®—
        total_reviews = len(df)
        unique_hospitals = df['hospital_id'].nunique()
        avg_star_rating = float(df['star_rating'].mean())
        star_distribution = df['star_rating'].value_counts().sort_index().to_dict()
        
        stats = {
            'total_reviews': total_reviews,
            'unique_hospitals': unique_hospitals,
            'avg_star_rating': avg_star_rating,
            'star_distribution': star_distribution
        }
        
        # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
        data_for_js = df.to_dict('records')
        
        print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {len(df)}ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        return jsonify({
            'success': True, 
            'data': data_for_js,
            'stats': stats,
            'message': f'ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰'
        })
        
    except Exception as e:
        print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({'success': False, 'error': f'ã‚µãƒ³ãƒ—ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/load_sample', methods=['POST'])
def load_sample():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ãƒ­ãƒ¼ãƒ‰"""
    global uploaded_data
    
    try:
        sample_file_path = os.path.join(os.path.dirname(__file__), 'sample_data.csv')
        
        if not os.path.exists(sample_file_path):
            return jsonify({'error': 'ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}), 400
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        df = pd.read_csv(sample_file_path)
        
        # å¿…è¦ãªåˆ—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        required_columns = ['hospital_id', 'review_text', 'star_rating']
        if not all(col in df.columns for col in required_columns):
            return jsonify({'error': f'å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {required_columns}'}), 400
        
        # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')
        df = df.dropna(subset=['star_rating'])
        df['star_rating'] = df['star_rating'].astype(int)
        
        # 1-5ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
        df = df[(df['star_rating'] >= 1) & (df['star_rating'] <= 5)]
        
        uploaded_data = df
        
        # åŸºæœ¬çµ±è¨ˆé‡è¨ˆç®—
        total_reviews = len(df)
        unique_hospitals = df['hospital_id'].nunique()
        avg_star_rating = float(df['star_rating'].mean())
        star_distribution = df['star_rating'].value_counts().sort_index().to_dict()
        
        print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰: ç·å£ã‚³ãƒŸæ•°={total_reviews}, ç—…é™¢æ•°={unique_hospitals}, å¹³å‡æ˜Ÿè©•ä¾¡={avg_star_rating:.2f}")
        print(f"ç—…é™¢IDä¸€è¦§: {df['hospital_id'].unique().tolist()}")
        
        stats = {
            'total_reviews': total_reviews,
            'unique_hospitals': unique_hospitals,
            'avg_star_rating': avg_star_rating,
            'star_distribution': star_distribution
        }
        
        return jsonify({'success': True, 'stats': stats, 'sample_loaded': True})
        
    except Exception as e:
        return jsonify({'error': f'ã‚µãƒ³ãƒ—ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/test_simple.html')
def test_simple():
    """Simple Canvas test page"""
    try:
        with open('test_simple.html', 'r', encoding='utf-8') as f:
            content = f.read()
        return content, 200, {'Content-Type': 'text/html; charset=utf-8'}
    except FileNotFoundError:
        return "Test file not found", 404

@app.route('/test_flow.html')
def test_flow():
    """Test analysis flow page"""
    try:
        with open('test_flow.html', 'r', encoding='utf-8') as f:
            content = f.read()
        return content, 200, {'Content-Type': 'text/html; charset=utf-8'}
    except FileNotFoundError:
        return "Test flow file not found", 404

@app.route('/test_direct.html')
def test_direct():
    """Direct test page simulating user experience"""
    try:
        with open('test_direct.html', 'r', encoding='utf-8') as f:
            content = f.read()
        return content, 200, {'Content-Type': 'text/html; charset=utf-8'}
    except FileNotFoundError:
        return "Test direct file not found", 404

if __name__ == '__main__':
    import os
    
    # Renderç”¨ã®ãƒãƒ¼ãƒˆè¨­å®š
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'
    
    logger.info(f"ğŸš€ Starting Flask app on port {port}")
    logger.info(f"ğŸ”§ Debug mode: {debug}")
    logger.info(f"ğŸ“Š Models configured: {len(MODELS)}")
    logger.info(f"ğŸ¯ Mock mode: {os.environ.get('USE_MOCK_MODELS', 'false')}")
    
    app.run(
        debug=debug, 
        host='0.0.0.0', 
        port=port,
        use_reloader=False  # Renderå¯¾å¿œ
    )