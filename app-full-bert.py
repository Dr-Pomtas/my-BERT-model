import os
import pandas as pd
import numpy as np
from flask import Flask, render_template, request, jsonify, send_file
import plotly
import plotly.graph_objs as go
import plotly.express as px
from sklearn.metrics import mean_absolute_error
from scipy.stats import pearsonr
from scipy import stats
import json
import re
import io
import base64
from werkzeug.utils import secure_filename
import random
import logging

# ãƒ•ãƒ«BERTãƒ¢ãƒ‡ãƒ«ç‰ˆï¼šå®Ÿéš›ã®Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    import torch
    BERT_AVAILABLE = True
    print("âœ… BERT ãƒ¢ãƒ‡ãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âŒ BERT ãƒ¢ãƒ‡ãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("pip install torch transformers ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    BERT_AVAILABLE = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

def convert_numpy_types(obj):
    """numpyå‹ã‚’Pythonãƒã‚¤ãƒ†ã‚£ãƒ–å‹ã«å¤‰æ›"""
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.bool_, bool)):
        return bool(obj)
    else:
        return obj

app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
uploaded_data = None
analysis_results = None

# ä½¿ç”¨ã™ã‚‹3ã¤ã®å®Ÿéš›ã®BERTãƒ¢ãƒ‡ãƒ«
MODELS = {
    'cl-tohoku/bert-base-japanese-whole-word-masking': 'Model A (Tohoku BERT)',
    'llm-book/bert-base-japanese-v3': 'Model B (LLM-book)',
    'Mizuiro-sakura/luke-japanese-base-finetuned-vet': 'Model C (Mizuiro Vet)'
}

class FullBertSentimentAnalyzer:
    def __init__(self):
        self.models = {}
        self.tokenizers = {}
        self.load_models()
    
    def load_models(self):
        """å®Ÿéš›ã®BERTãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if not BERT_AVAILABLE:
            print("âŒ BERTãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚pip install torch transformers ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return
        
        for model_name, display_name in MODELS.items():
            try:
                print(f"ğŸ“¥ {display_name} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                
                # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç•°ãªã‚‹è¨­å®šã‚’ä½¿ç”¨
                if 'tohoku' in model_name:
                    # æ±åŒ—å¤§å­¦BERTã®å ´åˆ
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = AutoModelForSequenceClassification.from_pretrained(model_name)
                elif 'llm-book' in model_name:
                    # LLM-bookBERTã®å ´åˆ  
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = AutoModelForSequenceClassification.from_pretrained(model_name)
                elif 'Mizuiro' in model_name:
                    # Mizuiroç£åŒ»ç‰¹åŒ–BERTã®å ´åˆ
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = AutoModelForSequenceClassification.from_pretrained(model_name)
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = AutoModelForSequenceClassification.from_pretrained(model_name)
                
                # GPUåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ä½¿ç”¨
                if torch.cuda.is_available():
                    model = model.cuda()
                    print(f"ğŸš€ {display_name} ã‚’GPUã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                else:
                    print(f"ğŸ’» {display_name} ã‚’CPUã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                
                self.tokenizers[model_name] = tokenizer
                self.models[model_name] = model
                
                print(f"âœ… {display_name} ã®èª­ã¿è¾¼ã¿å®Œäº†")
                
            except Exception as e:
                print(f"âŒ {display_name} ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ¢ãƒƒã‚¯åˆ†æã‚’ä½¿ç”¨
                print(f"ğŸ”„ {display_name} ã¯ãƒ¢ãƒƒã‚¯åˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
    
    def preprocess_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
        if pd.isna(text):
            return ""
        
        # æ–‡å­—åˆ—ã«å¤‰æ›
        text = str(text)
        
        # URLé™¤å»
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # çµµæ–‡å­—ã¨ç‰¹æ®Šè¨˜å·ã®é™¤å»ï¼ˆåŸºæœ¬çš„ãªæ—¥æœ¬èªæ–‡å­—ã€è‹±æ•°å­—ã€åŸºæœ¬çš„ãªè¨˜å·ã®ã¿æ®‹ã™ï¼‰
        text = re.sub(r'[^\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u3400-\u4DBFa-zA-Z0-9\sã€‚ã€ï¼ï¼Ÿ\.,!?()ï¼ˆï¼‰\-]', '', text)
        
        # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã®ç©ºç™½ã«
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def analyze_sentiment_real(self, text, model_name):
        """å®Ÿéš›ã®BERTãƒ¢ãƒ‡ãƒ«ã§æ„Ÿæƒ…åˆ†æ"""
        try:
            if model_name not in self.models:
                return self.analyze_sentiment_mock(text, model_name)
            
            tokenizer = self.tokenizers[model_name]
            model = self.models[model_name]
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                padding=True, 
                max_length=512
            )
            
            # GPUåˆ©ç”¨å¯èƒ½ãªå ´åˆ
            if torch.cuda.is_available() and next(model.parameters()).is_cuda:
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # æ¨è«–å®Ÿè¡Œ
            with torch.no_grad():
                outputs = model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            
            # CPUã«ç§»å‹•ã—ã¦ numpyå¤‰æ›
            predictions = predictions.cpu().numpy()[0]
            
            # 2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®å ´åˆï¼ˆnegative, positiveï¼‰
            if len(predictions) == 2:
                return {
                    'negative': float(predictions[0]),
                    'positive': float(predictions[1])
                }
            # 3ã‚¯ãƒ©ã‚¹åˆ†é¡ã®å ´åˆï¼ˆnegative, neutral, positiveï¼‰
            elif len(predictions) == 3:
                return {
                    'negative': float(predictions[0]),
                    'neutral': float(predictions[1]),
                    'positive': float(predictions[2])
                }
            else:
                # äºˆæœŸã—ãªã„ã‚¯ãƒ©ã‚¹æ•°ã®å ´åˆ
                return {'positive': 0.5, 'negative': 0.5}
                
        except Exception as e:
            print(f"å®ŸBERTåˆ†æã‚¨ãƒ©ãƒ¼ ({model_name}): {str(e)}")
            return self.analyze_sentiment_mock(text, model_name)
    
    def analyze_sentiment_mock(self, text, model_name):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ¢ãƒƒã‚¯åˆ†æ"""
        processed_text = self.preprocess_text(text)
        if not processed_text:
            return {'positive': 0.5, 'negative': 0.5}
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
        positive_words = ['è‰¯ã„', 'ã‚ˆã„', 'è¦ªåˆ‡', 'ä¸å¯§', 'å®‰å¿ƒ', 'ç´ æ™´ã‚‰ã—ã„', 'å„ªã—ã„', 'æ¸…æ½”', 'çš„ç¢º', 'é ¼ã‚Š']
        negative_words = ['æ‚ªã„', 'ã‚ã‚‹ã„', 'é«˜ã„', 'é•·ã„', 'ç‹­ã„', 'ä¸ä¾¿', 'ä¸ååˆ†', 'å¤ã„', 'ä¸å®‰']
        
        positive_count = sum(1 for word in positive_words if word in processed_text)
        negative_count = sum(1 for word in negative_words if word in processed_text)
        
        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹ç‰¹æ€§ã‚’æŒãŸã›ã‚‹
        model_variants = {
            'cl-tohoku/bert-base-japanese-whole-word-masking': 0.0,
            'llm-book/bert-base-japanese-v3': 0.1, 
            'Mizuiro-sakura/luke-japanese-base-finetuned-vet': -0.05
        }
        
        variant = model_variants.get(model_name, 0.0)
        
        # åŸºæœ¬ã‚¹ã‚³ã‚¢è¨ˆç®—
        base_positive = 0.4 + (positive_count * 0.15) - (negative_count * 0.1) + variant
        base_negative = 0.4 + (negative_count * 0.15) - (positive_count * 0.1) - variant
        
        # æ­£è¦åŒ–ï¼ˆ0-1ã®ç¯„å›²ã«åã‚ã‚‹ï¼‰
        total = base_positive + base_negative
        if total > 0:
            positive_prob = base_positive / total
            negative_prob = base_negative / total
        else:
            positive_prob = 0.5
            negative_prob = 0.5
        
        # ã‚ãšã‹ãªãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’è¿½åŠ ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰
        text_hash = hash(processed_text + model_name) % 1000
        noise = (text_hash / 1000 - 0.5) * 0.1
        
        positive_prob = max(0.0, min(1.0, positive_prob + noise))
        negative_prob = 1.0 - positive_prob
        
        return {
            'positive': positive_prob,
            'negative': negative_prob
        }
    
    def analyze_sentiment(self, text, model_name):
        """æ„Ÿæƒ…åˆ†æï¼ˆå®ŸBERTå„ªå…ˆã€å¤±æ•—æ™‚ãƒ¢ãƒƒã‚¯ï¼‰"""
        processed_text = self.preprocess_text(text)
        if not processed_text:
            return {'positive': 0.5, 'negative': 0.5}
        
        if BERT_AVAILABLE and model_name in self.models:
            return self.analyze_sentiment_real(processed_text, model_name)
        else:
            return self.analyze_sentiment_mock(processed_text, model_name)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆãƒ•ãƒ«BERTç‰ˆï¼‰
analyzer = FullBertSentimentAnalyzer()

def calculate_scores(data):
    """å…¨ãƒ¢ãƒ‡ãƒ«ã§ã®æ„Ÿæƒ…åˆ†æã¨ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    results = {}
    
    for model_name, display_name in MODELS.items():
        print(f"ãƒ¢ãƒ‡ãƒ« {display_name} ã§ã®åˆ†æé–‹å§‹...")
        
        model_scores = []
        for idx, row in data.iterrows():
            sentiment = analyzer.analyze_sentiment(row['review_text'], model_name)
            if sentiment:
                # å£ã‚³ãƒŸã‚¹ã‚³ã‚¢è¨ˆç®—: (P(pos) * 2) - (P(neg) * 2)
                review_score = (sentiment['positive'] * 2) - (sentiment['negative'] * 2)
                model_scores.append(review_score)
                # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®3ä»¶ã®åˆ†æçµæœã‚’å‡ºåŠ›
                if idx < 3:
                    print(f"  ã‚µãƒ³ãƒ—ãƒ« {idx}: text='{row['review_text'][:30]}...', pos={sentiment['positive']:.3f}, neg={sentiment['negative']:.3f}, score={review_score:.3f}")
            else:
                model_scores.append(0.0)
                if idx < 3:
                    print(f"  ã‚µãƒ³ãƒ—ãƒ« {idx}: æ„Ÿæƒ…åˆ†æå¤±æ•—")
        
        print(f"  {display_name} ã‚¹ã‚³ã‚¢ç¯„å›²: min={min(model_scores):.3f}, max={max(model_scores):.3f}, avg={sum(model_scores)/len(model_scores):.3f}")
        
        data[f'{display_name}_score'] = model_scores
        print(f"ãƒ¢ãƒ‡ãƒ« {display_name} ã®åˆ†æå®Œäº†")
    
    # æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢æ­£è¦åŒ–: (1-5) â†’ (-2 to +2)
    try:
        data['star_rating'] = pd.to_numeric(data['star_rating'], errors='coerce')
        data['star_score'] = data['star_rating'] - 3
        print(f"æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢æ­£è¦åŒ–å®Œäº†: {data['star_score'].dtype}")
    except Exception as e:
        print(f"æ˜Ÿè©•ä¾¡ã‚¹ã‚³ã‚¢æ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        data['star_rating'] = data['star_rating'].astype(str).str.extract(r'(\d+)').astype(float)
        data['star_score'] = data['star_rating'] - 3
    
    return data

def aggregate_by_hospital(data):
    """ç—…é™¢å˜ä½ã§ã®é›†è¨ˆ"""
    hospital_stats = data.groupby('hospital_id').agg({
        'star_score': 'mean',
        **{f'{display_name}_score': 'mean' for display_name in MODELS.values()}
    }).reset_index()
    
    review_counts = data.groupby('hospital_id').size().reset_index(name='review_count')
    hospital_stats = hospital_stats.merge(review_counts, on='hospital_id')
    
    return hospital_stats

# ä»¥ä¸‹ã€å…ƒã®app.pyã‹ã‚‰ãƒ«ãƒ¼ãƒˆé–¢æ•°ã‚’ã‚³ãƒ”ãƒ¼
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_file():
    global uploaded_data
    
    try:
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
        
        if file and file.filename.endswith('.csv'):
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            content = file.read()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦èª­ã¿è¾¼ã¿
            try:
                df = pd.read_csv(io.StringIO(content.decode('utf-8')))
            except UnicodeDecodeError:
                try:
                    df = pd.read_csv(io.StringIO(content.decode('shift_jis')))
                except UnicodeDecodeError:
                    df = pd.read_csv(io.StringIO(content.decode('cp932')))
            
            # å¿…è¦ãªåˆ—ã®ç¢ºèª
            required_columns = ['hospital_id', 'review_text', 'star_rating']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                return jsonify({
                    'error': f'å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {", ".join(missing_columns)}',
                    'required_columns': required_columns,
                    'found_columns': df.columns.tolist()
                }), 400
            
            # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
            if len(df) == 0:
                return jsonify({'error': 'ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™'}), 400
            
            if len(df) > 10000:
                return jsonify({'error': 'ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ã¯10,000ä»¶ã¾ã§ã§ã™'}), 400
            
            uploaded_data = df
            
            return jsonify({
                'message': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ',
                'rows': len(df),
                'columns': df.columns.tolist(),
                'sample_data': df.head(3).to_dict('records')
            })
        
        return jsonify({'error': 'CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„'}), 400
        
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({'error': f'ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'}), 500

if __name__ == '__main__':
    # å®Ÿè¡Œç’°å¢ƒã®ç¢ºèª
    print("ğŸ” å®Ÿè¡Œç’°å¢ƒãƒã‚§ãƒƒã‚¯:")
    print(f"  - BERT ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: {'âœ… åˆ©ç”¨å¯èƒ½' if BERT_AVAILABLE else 'âŒ æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«'}")
    
    if BERT_AVAILABLE:
        print(f"  - CUDA: {'âœ… åˆ©ç”¨å¯èƒ½' if torch.cuda.is_available() else 'ğŸ’» CPUä½¿ç”¨'}")
        if torch.cuda.is_available():
            print(f"  - GPU: {torch.cuda.get_device_name(0)}")
    
    print(f"  - ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼: {'ğŸ¤– ãƒ•ãƒ«BERTç‰ˆ' if BERT_AVAILABLE else 'ğŸ“ ãƒ¢ãƒƒã‚¯ç‰ˆ'}")
    
    # é–‹ç™ºç’°å¢ƒã§ã®å®Ÿè¡Œ
    debug = os.environ.get('FLASK_ENV') == 'development'
    port = int(os.environ.get('PORT', 5000))
    
    print(f"ğŸš€ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹:")
    print(f"  - URL: http://localhost:{port}")
    print(f"  - ãƒ¢ãƒ¼ãƒ‰: {'é–‹ç™º' if debug else 'æœ¬ç•ª'}")
    
    app.run(
        debug=debug, 
        host='0.0.0.0', 
        port=port,
        use_reloader=False
    )